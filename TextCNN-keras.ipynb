{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN\n",
    "In this jupyter notebook, We gave a simple implementation in [**Convolutional Neural Networks for Sentence Classification**](https://arxiv.org/pdf/1408.5882.pdf) with keras. You can use this notebook to reproduct the results of **\"CNN-rand\" and \"CNN-non-static\"** model in the paper above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Load and pre-process the data\n",
    "For simplicity, here we only load and process the [MR](http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz) data in [**Convolutional Neural Networks for Sentence Classification**](https://arxiv.org/pdf/1408.5882.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Import the package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import keras\n",
    "import keras.layers as L\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Hyperprameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperprameters\n",
    "USE_PRE_TRAIN_EMBEDDING = True\n",
    "EMBEDDING_DIM = 300\n",
    "POSITIVE_DATA_FILE = './rt-polaritydata/rt-polarity.pos'\n",
    "NEGATIVE_DATA_FILE = './rt-polaritydata/rt-polarity.neg'\n",
    "DEV_SAMPLE_PERCENTAGE = 0.1\n",
    "NUM_CLASSES = 2\n",
    "NUM_FILTERS = 128\n",
    "FILTER_SIZES = (3, 4, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load the MR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\", encoding='utf-8').readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\", encoding='utf-8').readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_text, y = load_data_and_labels(POSITIVE_DATA_FILE, NEGATIVE_DATA_FILE)\n",
    "print('Total records of the MR data set: ', len(x_text))\n",
    "max_doc_length = max([len(x.split(' ')) for x in x_text])\n",
    "print(\"Max document length: \", max_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for doc in x_text for t in doc.split(' ')]\n",
    "print(\"Total tokens in the MR data set: \", len(tokens))\n",
    "counter = Counter(tokens)\n",
    "index2word = list(counter.keys())\n",
    "index2word.insert(0, 'PAD')\n",
    "print(\"Vocabulary size in MR data set(contains 'PAD' as first): \", len(index2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_matrix(sequences, max_len, index2word):\n",
    "    matrix = np.full((len(sequences), max_len), 0)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        row_ix = [index2word.index(w) for w in seq.split(' ')]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix = as_matrix(x_text, max_doc_length, index2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_matrix, y, test_size=DEV_SAMPLE_PERCENTAGE)\n",
    "print('Train records: ', len(x_train))\n",
    "print('Test records:', len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load the pre-trained word2vec\n",
    "We use the publicly available word2vec vectors that were trained on 100 billion words from Google News. The vectors have dimensionality of 300 and were trained using the continuous bag-of-words architecture [(Mikolov et al., 2013)](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). Words not present in the set of pre-trained words are initialized randomly. You can download the Google pre-trained word2vec [Here]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pre_train_word2vec(model, index2word, vocab_size):\n",
    "    embedding_size = model.vector_size\n",
    "    pre_train_word2vec = dict(zip(model.vocab.keys(), model.vectors))\n",
    "    word_embedding_2dlist = [[]] * vocab_size    # [vocab_size, embedding_size]\n",
    "    word_embedding_2dlist[0] = np.zeros(embedding_size)    # assign empty for first word:'PAD'\n",
    "    pre_count = 0    # vocabulary in pre-train word2vec\n",
    "    # loop for all vocabulary, note that the first is 'PDA'\n",
    "    for i in range(1, vocab_size):\n",
    "        if index2word[i] in pre_train_word2vec:\n",
    "            word_embedding_2dlist[i] = pre_train_word2vec[index2word[i]]\n",
    "            pre_count += 1\n",
    "        else:\n",
    "            # initilaize randomly if vocabulary not exits in pre-train word2vec\n",
    "            word_embedding_2dlist[i] = np.random.uniform(-0.1, 0.1, embedding_size)\n",
    "    return np.array(word_embedding_2dlist), pre_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding, pre_count = get_pre_train_word2vec(word2vec_model, index2word, len(index2word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.TextCNN model\n",
    "The model is the same as [**Convolutional Neural Networks for Sentence Classification**](https://arxiv.org/pdf/1408.5882.pdf). Fig 1 shows the model architecture.(The Fig 1 is from [**A Sensitivity Analysis of (and Practitionersâ€™ Guide to) Convolutional Neural Networks for Sentence Classification**](https://arxiv.org/pdf/1510.03820.pdf))<br>\n",
    "![Fig 1 Text CNN](./textcnn.png)<br>\n",
    "<center>*Fig 1 Text CNN model architecture*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Function to Calculate the precison of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(model, x_test ,y_true):\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    y_predict = model.predict(x_test)\n",
    "    y_predict = np.argmax(y_predict, axis=1)\n",
    "    true_count = sum(y_true == y_predict)\n",
    "    return true_count / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Text CNN architecture in Fig 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cnn(sequence_length, num_classes, vocab_size, embedding_size, \n",
    "             filter_sizes, num_filters, embedding_matrix, drop_out=0.5 ,l2_reg_lambda=0.0):\n",
    "    input_x = L.Input(shape=(sequence_length,), name='input_x')\n",
    "    \n",
    "    # embedding layer\n",
    "    if embedding_matrix is None:\n",
    "        embedding = L.Embedding(vocab_size, embedding_size, name='embedding')(input_x)\n",
    "    else:\n",
    "        embedding = L.Embedding(vocab_size, embedding_size, weights=[embedding_matrix], name='embedding')(input_x)\n",
    "    expend_shape = [embedding.get_shape().as_list()[1], embedding.get_shape().as_list()[2], 1]\n",
    "    # embedding_chars = K.expand_dims(embedding, -1)    # 4D tensor [batch_size, seq_len, embeding_size, 1] seems like a gray picture\n",
    "    embedding_chars = L.Reshape(expend_shape)(embedding)\n",
    "    \n",
    "    # conv->max pool\n",
    "    pooled_outputs = []\n",
    "    for i, filter_size in enumerate(filter_sizes):\n",
    "        conv = L.Conv2D(filters=num_filters, \n",
    "                        kernel_size=[filter_size, embedding_size],\n",
    "                        strides=1,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        kernel_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1),\n",
    "                        bias_initializer=keras.initializers.constant(value=0.1),\n",
    "                        name=('conv_%d' % filter_size))(embedding_chars)\n",
    "        # print(\"conv-%d: \" % i, conv)\n",
    "        max_pool = L.MaxPool2D(pool_size=[sequence_length - filter_size + 1, 1],\n",
    "                               strides=(1, 1),\n",
    "                               padding='valid',\n",
    "                               name=('max_pool_%d' % filter_size))(conv)\n",
    "        pooled_outputs.append(max_pool)\n",
    "        # print(\"max_pool-%d: \" % i, max_pool)\n",
    "    \n",
    "    # combine all the pooled features\n",
    "    num_filters_total = num_filters * len(filter_sizes)\n",
    "    h_pool = L.Concatenate(axis=3)(pooled_outputs)\n",
    "    h_pool_flat = L.Reshape([num_filters_total])(h_pool)\n",
    "    # add dropout\n",
    "    dropout = L.Dropout(drop_out)(h_pool_flat)\n",
    "    \n",
    "    # output layer\n",
    "    output = L.Dense(num_classes,\n",
    "                     kernel_initializer='glorot_normal',\n",
    "                     bias_initializer=keras.initializers.constant(0.1),\n",
    "                     activation='softmax',\n",
    "                     name='output')(dropout)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input_x, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CNN-rand\n",
    "Here we run the **CNN-rand** model in the original paper. After running 10 epoches we can reach the accuracy at about 75%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rand = text_cnn(x_train.shape[1], NUM_CLASSES, len(index2word), EMBEDDING_DIM, FILTER_SIZES, NUM_FILTERS, None)\n",
    "cnn_rand.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_rand_history = cnn_rand.fit(x_train, y_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Evaluate the accuracy on dev set. After running 10 epoches we can reach about 75%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(cnn_rand, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 CNN-non-static\n",
    "Here we run the **CNN-non-static** model in the original paper. After running 10 epoches we can reach the accuracy at about 79%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_non_static = text_cnn(x_train.shape[1], NUM_CLASSES, len(index2word), EMBEDDING_DIM, FILTER_SIZES, NUM_FILTERS, word_embedding)\n",
    "cnn_non_static.compile('adam', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "cnn_non_static_history = cnn_non_static.fit(x_train, y_train, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **Evaluate the accuracy on dev set. After running 10 epoches we can reach about 75%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(cnn_non_static, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
